!pip install nltk
!pip install youtube-transcript-api scikit-learn nltk
import os
import numpy as np
from youtube_transcript_api import YouTubeTranscriptApi
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize
import nltk
nltk.download('punkt')
nltk.data.find('tokenizers/punkt')

# Ensure necessary NLTK data is available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

def get_video_transcript(video_url):
    "Extracts the transcript from a YouTube video."
    video_id = video_url.split("v=")[-1]
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
        full_text = "".join([x['text'] for x in transcript]).replace("\n", " ").strip()
        return full_text
    except Exception as e:
        print(f"Error fetching transcript: {e}")
        return None

def extractive_summarization(text, compression_ratio=0.5):
    "Summarizes the given text using extractive summarization, keeping important points."
    sentences = sent_tokenize(text)
    total_sentences = len(sentences)
    num_summary_sentences = max(1, int(total_sentences * compression_ratio))

    if total_sentences < 2:
        return "Transcript too short for summarization."

    # TF-IDF Vectorizer
    tf_idf = TfidfVectorizer(
        min_df=1,
        max_df=0.95,
        strip_accents='unicode',
        analyzer='word',
        lowercase=True,
        token_pattern=r'(?u)\b\w\w+\b',
        ngram_range=(1, 2),
        use_idf=True, smooth_idf=True,
        sublinear_tf=True,
        stop_words='english'
    )

    sentence_vectors = tf_idf.fit_transform(sentences)
    sent_scores = np.array(sentence_vectors.sum(axis=1)).ravel()
    top_sentence_indices = sent_scores.argsort()[-num_summary_sentences:][::-1]
    top_sentences = [sentences[i] for i in sorted(top_sentence_indices)]
    return " ".join(top_sentences)

# Example usage and add your URL of youTube video to run this code.
youtube_link = " "
transcript_text = get_video_transcript(youtube_link)

if transcript_text:
    summary = extractive_summarization(transcript_text, compression_ratio=0.5)
    print("\n **Summary:-**")
    print(" ".join(summary.split(". ")).replace(". ", ".\n\n"))
